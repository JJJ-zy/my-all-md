# 网络爬虫（Python）

## 请求头常用参数

```bash
1.user-agent:伪装爬虫作为浏览器发送请求
2.referer:表明当前的页面是从哪个url跳转过来的，反爬虫技术可以使用，必须通过指定页面跳转
3.cookie:做登录，或者识别请求
```

## 常见状态码

```bash
#请求正常，服务器返回正常的返回数据
	200   #识别到你是爬虫，有可能返回空
#永久重定向，比如在www.jingdong.com 会重定向到www.jd.com
	301
#临时重定向。比如在访问一个需要登陆的页面的时候，此时没有登陆，就会重定向到登录页面
	302
#请求的url在服务器上找不到
	400
#服务器拒绝访问，权限不够
	403
#服务器内部错误
	500
```

## urllib库

> urllib库是Python中的一个最基本的网络请求库，可以模拟浏览器的行为，向指定的服务器发送请求，并可以保存服务器返回的数据

> 在Python 3的urllib库中，所有和网络请求相关的方法，都被集中在urllib.request模块下

```python
from urllib import request
```

### urlopen函数

```python
from urllib import request
resp = request.urlopen("url")  #发送浏览器模拟请求获取页面
print(resp.read())

#urlopen参数详解，url:请求的url，data:请求的data,如果设置了这个值，就会变为post请求
#返回值是html.client.HTTPResponse对象，有read(size),readline,readlines和getcode等方法
```

### urlretrieve函数

> 这个函数可以方便的将网页上的一个文件保存到本地

```python
from urllib import request
request.urlretrieve("url","下载位置+文件名")
```

### urlencode和parse_qs函数

> 用浏览器发送请求时，url中包含中文浏览器会自动编码，我们使用的代码中需要我们自己进行编码

```python
from urllib import parse
data = {'name':'爬虫基础','greet':'hello world','age':100} #对需要带的参数编码，对象会自动转换为url格式
qs = parse.urlencode(data)
print(qs)
```

> 解码

```python
from urllib import parse
data = {'name':'爬虫基础','greet':'hello world','age':100} #对需要带的参数编码，对象会自动转换为url格式
qs = parse.urlencode(data)
print(qs)
print(parse.parse_qs(qs))
```

### urlsplit和urlparse

分割url，urlsplit和urlparse一样   urlparse多一个参数params

```python
# 导入urllib第三方库
from urllib import request, parse

# 创建url
url = "http://www.baidu.com/s?wd=liudehua"

# 分割url
result = parse.urlsplit(url)
# result = parse.urlparse(url)

print("scheme = ", result.scheme)  # 协议
print("netloc", result.netloc)  # ip地址
print("path", result.path)  # 访问路径
print("query", result.query)    # request参数
print("fragment", request.fragment) #锚点
```

### request. Request类

> 请求时添加请求头内容，比如增加User-Agent

```python
from urllib import request

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) 				Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.78'
}
req = request.Request("http://www.baidu.com",headers=headers)   # 生成带请求头的url
resp = request.urlopen(req)
print(resp.read())
```

### ProxyHandler处理器(代理设置)

> 很多网站会检查某一时间段某个IP的访问次数，如果访问次数太多，会禁止IP的访问，所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，换个IP依然可以访问，urllib可以使用PproxyHandler来设置代理服务器，下面代码说明如何自定义opener来使用代理

```python
from urllib import request

#使用代理
handler = request.ProxyHandler({"http":"218.66.161.88:31769"})

opener = request.build_opener(handler)
req = request.Request("http://httpbin.org/ip")
resp = opener.open(req)
print(resp.read())
```

> 常用的代理

```python
西拉免费代理IP: http://www.xiladaili.com/
快代理: http://www.kuaidaili.com/
代理云: http://www.dailiyun.com/
```

## Cookie

```python
#格式
	Set-Cookie: NAME=VALUE; Expires/Max-age=DATE; Path=PATH; Domain=DOMAIN_NAME; SECURE
#解释
	NAME: cookie的名字
    VALUE: cookie的值
    Expires: cookie的过期时间
    Path: cookie的路径
    Domain: cookie作用的域名
    SECURE: 是否只在https协议下起作用
```

### http.cookiejar模块

```tex
该模块主要的类有CookieJar,FileCookieJar,MozillaCookieJar,LWPCookieJar.这四个类的作用如下:
1.CookieJar:管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie对象。整个cookie都存储在内存中，对
	cookieJar实例进行垃圾回收后cookie也将失去
2.FileCookieJar(filename,delayload=None,policy=None):从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为true时支持延迟访问的文件，即只有在需要时才读取文件或在文件中存储数据
3.MozillaCookieJar(filename,delayload=None,policy=None):从FileCookieJar派生而来，创建与Mozilla浏览器cookies.txt兼容的FileCookieJar实例
4.LWPCookieJar(filename,delayload=None,policy=None):从FileCookieJar派生而来,创建与libwww-per标准的Set-Cookie3文件格式兼容的FileCookieJar实例

利用http.cookiejar 和 request.HTTPCookieProcessor登录人人网。相关代码如下
```

```python
from urllib import request,parse
from http.cookiejar import CookieJar

url = 'https://www.jianshu.com/sessions'

# 创建CookieJar对象
cookieJar = CookieJar()
# 使用cookiejar创建一个HTTPCookieProcessor对象,登录成功后会将cookie对象封装到cookieJar中
handler = request.HTTPCookieProcessor(cookieJar)
# 使用handler创建一个opener
opener = request.build_opener(handler)
# 发送登录请求（账号和密码）
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.78 '
}
data = {
    'session[email_or_mobile_number]': '9701380749@qq.com',
    'session[password]': 'pythonSpider'
}
req = request.Request(url, data=parse.urlencode(data).encode("utf-8"), headers=headers)
opener.open(req)

# 访问个人主页
self_url = 'https://www.jianshu.com/u/81066a046056'
req2 = request.Request(self_url, headers=headers)
reps = opener.open(req2)

with open("jianshu.html", "w", encoding="utf-8") as fp:
    fp.write(reps.read().decode("utf-8"))
```

### 保存Cookie到本地

> save方法,需要指定文件名
>
> 自己的cookie默认是浏览器会话结束后过期，代码请求完会话就结束了，所以自己的cookie保存不上，需要设置ignore_discard=True

```python
from urllib import request
from http.cookiejar import MozillaCookieJar

cookiejar = MozillaCookieJar("cookie.txt")
handler = request.HTTPCookieProcessor(cookiejar)
opener = request.build_opener(handler)

headers = {
    
}
req = request.Request('http://httpbin.org/cookies',headers=headers)

resp = opener.open(req)
print(resp.read())
cookiejar.save(ignore_discard=True,ignore_expires=True)
```

### 读取本地Cookie

> load方法，需要指定文件名

>ignore_discard=True

```python
from urllib import request
from http.cookiejar import MozillaCookieJar

cookiejar = MozillaCookieJar("cookie.txt")
cookiejar.load(ignore_discard=True)
handler = request.HTTPCookieProcessor(cookiejar)
opener = request.build_opener(handler)

headers = {
    
}
req = request.Request('http://httpbin.org/cookies',headers=headers)

resp = opener.open(req)
print(resp.read())
for cookie in cookiejar:
    print(cookie)
```

## Requests库

### 请求方式

```python
import requests
# 发送什么请求就用什么函数
response = requests.get(url)
response = requests.post(url)
```

### 添加headers和查询参数

> 如果想添加headers,可以传入headers参数来增加请求头中的headers信息。如果要将参数在url中传递，可以利用params参数
>
> data参数也自动编码不需要手动编码，直接放到请求函数中

```python
import requests

kw = ('wd': '中国')

headers = {'User-Agent': ''}

# params接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要使用urlencode()
# 自动将params编码后拼接到url后面http://www.baidu.com/s?wd=%E4%B8%AD%E5%9B%BD
response = requests.get(url, params = kw, headers = headers)
#查看响应内容，response.text 返回的是Unicode格式的数据
print(response.text)             # 是按默认解码方式对request.content(网页源代码)解码得到的

#查看响应内容，response.content返回的字节流数据,可以解码输出看得懂的数据
print(response.content.encode('utf-8'))

# 查看完整的url地址
print(response.url)

#查看响应头部字符编码
print(response.encoding)

#查看响应码
print(response.status_code)
```

### 使用代理

> 只需要早请求函数中传递proxies参数

```python
import requests

url = 'http://httpbin.org/get'

header = {
    'User-Agent': ''
}

#proxy = {
#    'http': '127.0.0.1:8080'
#}
# Python3.7及以上 必须加上http://，不加就会报错
proxies = {
    'http': 'http://39.105.128.50:8080'
}

resp = requests.get(url, headers=headers, proxies=proxy)
print(resp.content)
```

### cookie

> 如果在一个响应中包含了cookie,那么可以利用cookies属性拿到这个返回的cookie值

```python
import requests

url = ""
data={}
resp = requests.get(url,header,data)
print(resp.cookie)
```

#### session

> 共享cookie

```python
import requests

url = ""
data={}
headers={}

#登录
session = requests.session()
session.post(url,data,headers)

#访问个人中心
resp = session.get(newurl)

print(resp.text)
```

### 处理不信任的SSL证书

> 对于已经被信任ssl证书的网站，可以直接返回响应。
>
> 而不信任的需要带参数告诉python不需要检查ssl证书 verify=false

```python
resp = requests.get(url,verify=false)
```

## XPath语法和lxml模块

> xpath概念

```tex
xpath(XML Path Language)是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历
```

> xpath开发工具

```tex
1.Chrome插件XPath Helper
2.Firefox插件XPath Checker  / try xpath
```

### XPath语法

> XPath使用路径表达式来选取XML文档中的节点或者节点集。这些路径表达式和我们在常规的电脑系统中看到的表达式相似

```bash
# 选取此节点的所有节点
nodename      # 例如   bookstore  选取bookstore下的所有子节点

# 如果是在最前面，代表从根节点选取，否则选择某节点的下个节点
/		# 例如  /bookstore	选取根元素下的所有bookstore节点

#从全局节点中选择节点，随便在哪个位置
//		# 例如   //book	从全局节点中找到所有的book节点

#  选取某个节点的属性
@		# 例如   //book[@class]	选择所有book节点的classs属性
```

#### 谓词

> 谓语用来查找某个特定节点或者某个指定值的节点，被嵌在方括号中

```bash
# 选取bookstore下的第一个子元素
/bookstore/book[1]

# 选取bookestore下的最后一个book元素
/bookstore/book[last()]

#选取bookstore下的前两个子元素
/bookstore/book[position()<3]

# 选取拥有price属性的book元素
//book[@price]

#选取所有属性price等于10的元素
//book[@price=10]

#模糊匹配
//book[contains(@class,'f1')]
```

#### 通配符

```bash
# 匹配任意节点
*		#   /bookstore/*   # 选取bookstore下的所有子元素

# 匹配节点中的任何属性
@*		#	//book[@*]	选取所有带有属性的book元素	
```

#### 选取多个路径

```bash
# 选取所有book元素和book元素下的所有title元素
	|
//bookstore/book | //book/title
```

#### 运算符

![image-20210828133058678](C:\Users\zwj\AppData\Roaming\Typora\typora-user-images\image-20210828133058678.png)

![image-20210828133122733](C:\Users\zwj\AppData\Roaming\Typora\typora-user-images\image-20210828133122733.png)

### lxml库

> lxml是一个HTML/XML的解析器，主要的功能是如何解析和提取HTML/XML数据

> lxml和正则一样，也是用C语言实现的，是一款高性能的Python HTML/XML解析器，我们可以利用之前学习的XPath语法，来快捷的定位特定元素以及节点信息

> 就是将爬取下来的页面字符串包装成html对象,方便xpath提取数据

#### 基本使用

> 解析html代码，代码不规范时会自动补全

```python
# 使用 lxml 的 etree 库
from lxml import etree
text = '''
	<div>
    	<ul>
    		<li class="item-0"><a href="link.html">first item</a></li>
    		<li class="item-1"><a href="link2.html">second item</a></li>
    		<li class="item-inactive"><a href="link3.html">third item</a></li>
    		<li class="item-1"><a href="link4.html">fourth item</a></li>
    		<li class="item-0"><a href="link5.html">fifth item</a>     # 此处缺少闭合标签</li>
    	</ul>
    </div>
'''

# 利用etree.HTML,将字符串解析为HTML文档
html = etree.HTML(text)

# 按字符串序列化HTML文档
result = etree.tostring(html)

print(result)
```

> 加载外部html文件

```python
etree.parse('xxx.html')	# 不会自动补全，需要传递解析器

# 设置解析器
parser = etree.HtmlParser(encoding='utf-8')
etree.parse('xxx.html',parser=parser)

```

> 编码

```python
# 编码
html = etree.HTML(text,encoding='utf-8')

# 解码
etree.tostring(html,encoding='utf-8').decode('utf-8')
```

### 联合使用

```python
# 获取属性值
xpath("//book/@class")

# 获取标签内文本
xpath("//book/text()")

# 子级元素
xpath(".//book")   |  xpath("./book/aa")
```

